% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Maxcombo_size.R
\name{GSMC_design}
\alias{GSMC_design}
\title{Predicted sample sizes and boundaries for GSMC design}
\usage{
GSMC_design(
  FHweights,
  interim_ratio,
  error_spend,
  eps,
  p,
  b,
  tau,
  omega,
  lambda,
  lambda.trt,
  rho,
  gamma,
  beta,
  stoch = TRUE,
  range_ext = 200,
  time_ext_multiplier = 1.5,
  time_increment = 0.01,
  n.rep = 5
)
}
\arguments{
\item{FHweights}{a list of pairs of parameters (rho, gamma) for the Fleming Harrington weighted log-rank tests:\eqn{W(t)=S^\rho(t^-)(1-S(t^-))^\gamma}. The first one will provide the information fraction we will use to sign the stopping time. Now we only accommodate the surrogate information fraction, i.e., rho = 0 and gamma = 0.}

\item{interim_ratio}{a vector of ratios between 0 and 1 that sign the stopping time following the first type of information fraction in FHweights, i.e. the event sizes. Note that the last value must be 1, otherwise, the function will automatically append 1 for the final stage.}

\item{error_spend}{cumulative errors spent at each stage (including the final one). Must be of the same length as interim_ratio or one element shorter if the last value of interim_ratio is less than 1. The last element of error_spend should be type I error \code{alpha}.}

\item{eps}{the change point, before which, the hazard ratio is 1, and after which, the hazard ratio is theta}

\item{p}{treatment assignment probability.}

\item{b}{the number of subintervals per time unit.}

\item{tau}{the end of the follow-up time  in the study. Note that this is identical to \eqn{T+\tau} in the paper from Hasegawa (2014).}

\item{omega}{the minimum follow-up time for all the patients.  Note that Hasegawa(2014) assumes that the accrual is uniform between time 0 and R, and there does not exist any censoring except for the administrative censoring at the ending time \eqn{\tau}. Thus this value omega is equivalent to \code{tau-R}. Through our simulation tests, however, we found that this function is quite robust to violations of these assumptions: dropouts, different censoring rates for two  arms, and changing accrual rates.}

\item{lambda}{the hazard for the control group.}

\item{lambda.trt}{the hazard for the treatment group after time eps.}

\item{beta}{type II error, 1-power.}

\item{stoch}{stochastic prediction or not (exact prediction), the former is default.}

\item{range_ext}{the width to extend the range of sample sizes. If the predicted sample size is not found in the range, try a wider \code{range_ext}. It will automatically search for the likely range, but still possibly miss the best one. Its default value is 200.}

\item{time_ext_multiplier}{compute the time window for the possible stopping time points by multiplying it with the total expected follow-up time \code{tau}. The default is 1.5. In other words, when tau = 18, the longest time we consider would be \eqn{18 * 1.5 = 27} months.}

\item{time_increment}{time increments to compute the predicted stopping time points, the finer the more accurate.}

\item{n.rep}{same as the one in \code{\link{Maxcombo.sz}}. The number of repeats to take the median for output since the called likelihood generator of a multivariate normal distribution \code{\link[mvtnorm]{pmvnorm}} is not determinant. The default \code{n.rep} value is 5.}
}
\value{
\item{z_alpha_pred}{predicted boundary values for all the stages, length is equivalent to the input \code{interim_ratio} or \code{error_spend}.}
\item{z_alpha_vec_pred}{predicted boundary values for all the test statistics following \code{index}.}
\item{d_fixed}{the required observed events at each stage. }
\item{n_FH}{the total required number of subjects according to the defined hazards.}
\item{n_event_FH}{the total required number of events according to the defined hazards.}
\item{index}{records the ordered stages for each tests, starting from 1 and ending at the length of \code{interim_ratio} or \code{error_spend}. It is actually \code{rep(1:length(interim_ratio), each = length(FHweights))}. }
\item{interim_pred0}{predicts stopping time points under the null hypothesis following the order of \code{index}.}
\item{interim_pred1}{predicts stopping time points under the alternative hypothesis following the order of \code{index}.}
\item{Sigma0}{predicts correlation matrix under the null hypothesis with each row and column following the test statistics corresponding to \code{index}. }
\item{Sigma1}{predicts correlation matrix under the alternative hypothesis with each row and column following the test statistics corresponding to \code{index}. }
\item{mu1}{the predicts unit mean under the alternative hypothesis, the \eqn{\tilde{\mu}} in formula (5) of the reference paper. The test statistics follow \code{index}.  It is also the mean of the expectation of each subject scaled weighted log-rank test statistic, which can be approximated using the formula for \eqn{E^*} in Hasegawa 2014 paper. Under null, the predicted mean is otherwise 0, implying no treatment effect.  }
\item{stoch}{input \code{stoch} boolean variable, \code{TRUE} if stochastic prediction is enabled, \code{FALSE} otherwise. The default is \code{TRUE}. }
\item{FHweights}{input \code{FHweights} list. }
\item{interim_ratio}{input \code{interim_ratio} vector.}
\item{error_spend}{input \code{error_spend} vector. }
}
\description{
Compute predicted sample size and boundaries for a group sequential design of max-combo tests.
}
\details{
Predict the sample sizes and boundaries to achieve the targeted type I errors (error spent at each stage) and power. Prediction approaches include the exact prediction or the stochastic prediction approach following 2-piece-wise exponential distributions given in the appendix of the reference paper.
}
\examples{
\dontrun{
### Parameters
FHweights <- list(
  c(0,0),
  c(0,1),
  c(1,0)
  )
  n_FHweights <- length(FHweights)
  # stop when what proportion of the events have been observed.
  fixed_death_p <-  c(0.6, 0.7, 0.8)
  interim_ratio <- c(fixed_death_p,1)
  n_stage <- length(interim_ratio)
  # treatment assignment.
  p <- 1/2 
  # end of the study, chronological assuming it's on the alternative arm.
  tau <- 18
  # end of the accrual period, chronological.
  R <- 14 
  # minimum potential follow-up time, not considering dropouts or other subject-specific censoring.
  omega <- (tau-R) 
  # waiting time before the change point: can be the delayed effect or the crossing effect
  eps <- 2
  # event hazard of the control arm.
  lambda <- log(2)/6 
  # hazard ratio after the change point eps under the alternative hypothesis. 
  theta <- 0.6 
  # event hazard for the treatment arm under the alternative hypothesis and after the change point eps.
  lambda.trt <- lambda*theta  
  # type I error under the control.
  alpha <- 0.025 
  # type II error under the alternative. 
  beta <- 0.1 
  # Obtain the cumulative errors spent at each stage
  error_spend <- c(0.005, 0.01, 0.015, alpha)
  # number of subintervals for a time unit
  b <- 30
  res <- GSMC_design(
  FHweights,
  interim_ratio,
  error_spend,
  eps, 
  p, 
  b, 
  tau,]
  omega,
  lambda,
  lambda.trt,
  rho, 
  gamma,
  beta, 
  stoch = F
  )
}

}
\references{
Wang, L., Luo, X., & Zheng, C. (2021). A Simulation-free Group Sequential Design with Max-combo Tests in the Presence of Non-proportional Hazards. Journal of Pharmaceutical Statistics.
}
\author{
Lili Wang
}
